{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,AutoConfig,AutoModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = pd.read_csv(\"open/train.csv\",encoding='utf-8')\n",
    "selected_columns = data.loc[:, ['Question', 'Answer']]\n",
    "tds = Dataset.from_pandas(selected_columns)\n",
    "ds = DatasetDict()\n",
    "ds['train'] = tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 496/496 [00:00<00:00, 15993.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def create_text_column(example):\n",
    "    # 'text' 컬럼 생성\n",
    "    text = f\"### Instruction:\\n{example['Question']}\\n\\n### Response:\\n{example['Answer']}\"\n",
    "    example[\"text\"] = text\n",
    "    return example\n",
    "\n",
    "# 'text' 컬럼 생성\n",
    "datasethk = ds.map(create_text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### Instruction:\\n2024년 중앙정부 재정체계는 어떻게 구성되어 있나요?\\n\\n### Response:\\n2024년 중앙정부 재정체계는 예산(일반·특별회계)과 기금으로 구분되며, 2024년 기준으로 일반회계 1개, 특별회계 21개, 기금 68개로 구성되어 있습니다.',\n",
       " '### Instruction:\\n2024년 중앙정부의 예산 지출은 어떻게 구성되어 있나요?\\n\\n### Response:\\n2024년 중앙정부의 예산 지출은 일반회계 356.5조원, 21개 특별회계 81.7조원으로 구성되어 있습니다.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasethk['train']['text'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:25<00:00,  6.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## set base model\n",
    "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "#new_model = \"/content/gdrive/MyDrive/Colab Notebooks/llama/02. Fine Tuning/llama3_meta_hkcode_0602\"\n",
    "\n",
    "### Load basemodel\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "baseModel = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 496/496 [00:00<00:00, 14217.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "### Load basemodel's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# Must add EOS_TOKEN at response last line\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def prompt_eos(sample):\n",
    "    sample['text'] = sample['text']+EOS_TOKEN\n",
    "    return sample\n",
    "datasethk = datasethk.map(prompt_eos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:185: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 496/496 [00:00<00:00, 17656.93 examples/s]\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=datasethk['train'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    peft_config=lora_config,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        num_train_epochs = 2,\n",
    "        max_steps=1000,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_ratio=0.03,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  3%|▎         | 33/1000 [10:04<6:13:21, 23.17s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:451\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 451\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3324\u001b[0m ):\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\accelerate\\utils\\operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\accelerate\\utils\\operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\peft\\peft_model.py:1577\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1576\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1588\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1141\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1138\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1141\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:944\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    932\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    933\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    934\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    941\u001b[0m         position_embeddings,\n\u001b[0;32m    942\u001b[0m     )\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 944\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    955\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:677\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    690\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:562\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m    561\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m--> 562\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    565\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:556\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[1;32m--> 556\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m lora_B(\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    558\u001b[0m     x \u001b[38;5;241m=\u001b[39m dropout(x)\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1405: UserWarning: Current model requires 4224 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.84s/it]\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1405: UserWarning: Current model requires 8448 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ADAPTER_MODEL = \"llama3_lora\"\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)\n",
    "\n",
    "new_basemodel  = AutoModelForCausalLM.from_pretrained(base_model, device_map='auto', torch_dtype=torch.float16)\n",
    "loramodel  = PeftModel.from_pretrained(new_basemodel, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "mergedModel = loramodel.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\HANMAC\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|          | 1.90M/4.98G [00:00<04:49, 17.2MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|          | 4.69M/4.98G [00:00<04:19, 19.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|          | 6.59M/4.98G [00:00<15:16, 5.42MB/s]\n",
      "model-00001-of-00004.safetensors:   0%|          | 8.29M/4.98G [00:01<12:01, 6.89MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|          | 11.5M/4.98G [00:01<07:37, 10.8MB/s]\n",
      "model-00001-of-00004.safetensors:   0%|          | 14.5M/4.98G [00:01<05:49, 14.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|          | 16.7M/4.98G [00:01<10:02, 8.23MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|          | 19.1M/4.98G [00:01<08:09, 10.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|          | 21.9M/4.98G [00:02<06:42, 12.3MB/s]\n",
      "model-00001-of-00004.safetensors:   0%|          | 23.8M/4.98G [00:02<06:08, 13.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   1%|          | 25.7M/4.98G [00:02<09:49, 8.39MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|          | 27.1M/4.98G [00:02<10:06, 8.16MB/s]\n",
      "model-00001-of-00004.safetensors:   1%|          | 29.6M/4.98G [00:02<07:49, 10.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   1%|          | 32.0M/4.98G [00:03<12:17, 6.71MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   1%|          | 37.1M/4.98G [00:03<07:00, 11.7MB/s]\n",
      "model-00001-of-00004.safetensors:   1%|          | 44.2M/4.98G [00:03<04:04, 20.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|          | 48.0M/4.98G [00:04<04:51, 16.9MB/s]\n",
      "model-00001-of-00004.safetensors:   1%|▏         | 63.9M/4.98G [00:04<02:23, 34.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|▏         | 69.2M/4.98G [00:04<03:44, 21.9MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   1%|▏         | 73.2M/4.98G [00:05<05:26, 15.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 76.2M/4.98G [00:05<06:08, 13.3MB/s]\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 78.6M/4.98G [00:05<05:52, 13.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 80.8M/4.98G [00:06<09:11, 8.88MB/s]\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 86.3M/4.98G [00:06<06:44, 12.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 88.8M/4.98G [00:07<08:50, 9.21MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 90.3M/4.98G [00:07<09:23, 8.68MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 94.6M/4.98G [00:07<07:01, 11.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 96.1M/4.98G [00:08<11:57, 6.81MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 106M/4.98G [00:08<04:40, 17.4MB/s] \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|▏         | 121M/4.98G [00:08<03:08, 25.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 128M/4.98G [00:09<04:02, 20.0MB/s]\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 143M/4.98G [00:09<02:22, 34.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 149M/4.98G [00:10<03:50, 21.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 153M/4.98G [00:10<05:07, 15.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 159M/4.98G [00:11<05:20, 15.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 161M/4.98G [00:11<08:01, 10.0MB/s]\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 168M/4.98G [00:11<05:17, 15.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▎         | 172M/4.98G [00:11<04:16, 18.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▎         | 185M/4.98G [00:12<03:05, 25.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   4%|▍         | 192M/4.98G [00:12<03:30, 22.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   4%|▍         | 197M/4.98G [00:12<02:59, 26.6MB/s]\n",
      "model-00001-of-00004.safetensors:   4%|▍         | 206M/4.98G [00:12<02:07, 37.5MB/s]\n",
      "model-00001-of-00004.safetensors:   4%|▍         | 220M/4.98G [00:13<02:15, 35.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   5%|▍         | 226M/4.98G [00:13<03:13, 24.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   5%|▍         | 234M/4.98G [00:13<02:23, 33.1MB/s]\n",
      "model-00001-of-00004.safetensors:   5%|▍         | 240M/4.98G [00:13<02:09, 36.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   5%|▍         | 245M/4.98G [00:14<02:43, 29.0MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   5%|▌         | 254M/4.98G [00:14<02:01, 39.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▌         | 270M/4.98G [00:14<01:57, 40.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▌         | 276M/4.98G [00:15<03:17, 23.8MB/s]\n",
      "model-00001-of-00004.safetensors:   6%|▌         | 281M/4.98G [00:15<02:55, 26.8MB/s]\n",
      "model-00001-of-00004.safetensors:   6%|▌         | 286M/4.98G [00:15<02:37, 29.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   6%|▌         | 291M/4.98G [00:15<03:16, 23.9MB/s]\n",
      "model-00001-of-00004.safetensors:   6%|▌         | 300M/4.98G [00:16<02:14, 34.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   6%|▋         | 312M/4.98G [00:16<02:29, 31.2MB/s]\n",
      "model-00001-of-00004.safetensors:   6%|▋         | 318M/4.98G [00:16<02:16, 34.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   6%|▋         | 322M/4.98G [00:17<03:20, 23.3MB/s]\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 327M/4.98G [00:17<02:57, 26.2MB/s]\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 331M/4.98G [00:17<02:43, 28.5MB/s]\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 335M/4.98G [00:17<02:33, 30.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 339M/4.98G [00:17<03:25, 22.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 344M/4.98G [00:17<02:46, 27.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 347M/4.98G [00:17<02:39, 28.9MB/s]\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 351M/4.98G [00:17<02:33, 30.1MB/s]\n",
      "model-00001-of-00004.safetensors:   7%|▋         | 365M/4.98G [00:18<02:03, 37.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8%|▊         | 377M/4.98G [00:18<02:23, 32.2MB/s]\n",
      "model-00001-of-00004.safetensors:   8%|▊         | 384M/4.98G [00:19<03:00, 25.5MB/s]\n",
      "model-00001-of-00004.safetensors:   8%|▊         | 398M/4.98G [00:19<01:59, 38.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8%|▊         | 420M/4.98G [00:20<02:42, 28.1MB/s]\n",
      "model-00001-of-00004.safetensors:   9%|▊         | 430M/4.98G [00:20<01:59, 37.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▉         | 436M/4.98G [00:20<02:36, 28.9MB/s]\n",
      "model-00001-of-00004.safetensors:   9%|▉         | 444M/4.98G [00:20<02:03, 36.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   9%|▉         | 450M/4.98G [00:21<02:50, 26.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   9%|▉         | 458M/4.98G [00:21<02:11, 34.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▉         | 477M/4.98G [00:21<02:04, 36.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  10%|▉         | 491M/4.98G [00:22<02:12, 34.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▉         | 497M/4.98G [00:22<03:09, 23.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  10%|█         | 503M/4.98G [00:22<02:33, 29.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  10%|█         | 512M/4.98G [00:23<02:46, 26.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  10%|█         | 516M/4.98G [00:23<02:34, 28.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  10%|█         | 520M/4.98G [00:23<02:24, 30.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|█         | 528M/4.98G [00:23<02:54, 25.4MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|█         | 533M/4.98G [00:24<02:34, 28.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|█         | 538M/4.98G [00:24<02:13, 33.1MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|█         | 543M/4.98G [00:24<02:02, 36.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|█         | 548M/4.98G [00:24<02:51, 25.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|█         | 559M/4.98G [00:24<02:00, 36.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|█▏        | 564M/4.98G [00:25<03:01, 24.4MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|█▏        | 569M/4.98G [00:25<02:31, 29.1MB/s]\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 574M/4.98G [00:25<02:19, 31.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 578M/4.98G [00:25<03:12, 22.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 588M/4.98G [00:25<02:18, 31.7MB/s]\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 592M/4.98G [00:25<02:11, 33.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 596M/4.98G [00:26<03:09, 23.1MB/s]\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 602M/4.98G [00:26<02:29, 29.2MB/s]\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 606M/4.98G [00:26<02:20, 31.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|█▏        | 619M/4.98G [00:26<02:09, 33.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 624M/4.98G [00:27<03:11, 22.7MB/s]\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 628M/4.98G [00:27<02:51, 25.4MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 634M/4.98G [00:27<02:20, 30.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 640M/4.98G [00:27<02:56, 24.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 645M/4.98G [00:28<02:31, 28.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 649M/4.98G [00:28<02:23, 30.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 653M/4.98G [00:28<02:18, 31.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 657M/4.98G [00:28<03:33, 20.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 661M/4.98G [00:28<03:00, 24.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 664M/4.98G [00:28<02:47, 25.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 668M/4.98G [00:28<02:40, 26.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  13%|█▎        | 671M/4.98G [00:29<02:32, 28.2MB/s]\n",
      "model-00001-of-00004.safetensors:  14%|█▎        | 674M/4.98G [00:29<03:55, 18.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|█▎        | 678M/4.98G [00:29<03:17, 21.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|█▎        | 681M/4.98G [00:29<03:02, 23.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 684M/4.98G [00:29<02:50, 25.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 687M/4.98G [00:29<02:43, 26.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 690M/4.98G [00:30<04:34, 15.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 695M/4.98G [00:30<03:32, 20.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 698M/4.98G [00:30<03:12, 22.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 701M/4.98G [00:30<03:06, 22.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 703M/4.98G [00:30<03:01, 23.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 706M/4.98G [00:30<04:14, 16.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 715M/4.98G [00:30<02:16, 31.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|█▍        | 719M/4.98G [00:31<02:23, 29.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|█▍        | 734M/4.98G [00:31<02:03, 34.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|█▍        | 745M/4.98G [00:32<02:26, 28.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  15%|█▌        | 762M/4.98G [00:32<02:06, 33.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|█▌        | 777M/4.98G [00:33<02:05, 33.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|█▌        | 784M/4.98G [00:33<02:41, 26.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  16%|█▌        | 789M/4.98G [00:33<02:30, 27.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  16%|█▌        | 793M/4.98G [00:33<02:21, 29.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|█▌        | 800M/4.98G [00:34<02:44, 25.4MB/s]\n",
      "model-00001-of-00004.safetensors:  16%|█▌        | 805M/4.98G [00:34<02:24, 28.9MB/s]\n",
      "model-00001-of-00004.safetensors:  16%|█▋        | 809M/4.98G [00:34<02:13, 31.2MB/s]\n",
      "model-00001-of-00004.safetensors:  16%|█▋        | 814M/4.98G [00:34<02:03, 33.8MB/s]\n",
      "model-00001-of-00004.safetensors:  16%|█▋        | 818M/4.98G [00:34<02:51, 24.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 822M/4.98G [00:34<02:34, 26.9MB/s]\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 825M/4.98G [00:34<02:25, 28.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 829M/4.98G [00:35<02:17, 30.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 843M/4.98G [00:35<01:54, 36.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 848M/4.98G [00:35<02:36, 26.3MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 853M/4.98G [00:35<02:27, 28.1MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 857M/4.98G [00:36<02:10, 31.5MB/s]\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 862M/4.98G [00:36<01:54, 35.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|█▋        | 867M/4.98G [00:36<02:42, 25.2MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 876M/4.98G [00:36<02:06, 32.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 880M/4.98G [00:37<02:56, 23.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 894M/4.98G [00:37<01:54, 35.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 898M/4.98G [00:37<02:50, 23.9MB/s]\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 908M/4.98G [00:37<02:05, 32.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 912M/4.98G [00:38<03:02, 22.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 916M/4.98G [00:38<02:43, 24.9MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  18%|█▊        | 919M/4.98G [00:38<02:38, 25.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|█▊        | 927M/4.98G [00:38<02:14, 30.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|█▊        | 930M/4.98G [00:38<03:28, 19.5MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|█▉        | 939M/4.98G [00:39<02:20, 28.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|█▉        | 957M/4.98G [00:39<02:02, 32.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|█▉        | 969M/4.98G [00:40<02:05, 31.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|█▉        | 974M/4.98G [00:40<01:55, 34.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|█▉        | 989M/4.98G [00:40<01:54, 34.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|█▉        | 994M/4.98G [00:41<03:11, 20.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|██        | 998M/4.98G [00:41<02:48, 23.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|██        | 1.00G/4.98G [00:41<02:32, 26.0MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|██        | 1.01G/4.98G [00:41<02:24, 27.4MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|██        | 1.01G/4.98G [00:42<03:17, 20.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|██        | 1.01G/4.98G [00:42<02:51, 23.1MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|██        | 1.02G/4.98G [00:42<02:36, 25.3MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.03G/4.98G [00:42<03:33, 18.5MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.03G/4.98G [00:42<02:34, 25.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.03G/4.98G [00:42<02:21, 27.9MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.04G/4.98G [00:43<02:13, 29.5MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.04G/4.98G [00:43<03:13, 20.3MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.05G/4.98G [00:43<02:27, 26.7MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.05G/4.98G [00:43<02:02, 32.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|██        | 1.06G/4.98G [00:43<03:07, 20.9MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██▏       | 1.06G/4.98G [00:44<02:30, 25.9MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|██▏       | 1.07G/4.98G [00:44<02:06, 30.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|██▏       | 1.07G/4.98G [00:44<01:57, 33.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|██▏       | 1.08G/4.98G [00:44<03:09, 20.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|██▏       | 1.08G/4.98G [00:44<02:39, 24.5MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|██▏       | 1.08G/4.98G [00:44<02:29, 26.1MB/s]\n",
      "model-00001-of-00004.safetensors:  22%|██▏       | 1.09G/4.98G [00:45<02:20, 27.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|██▏       | 1.10G/4.98G [00:47<04:53, 13.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|██▏       | 1.12G/4.98G [00:47<02:56, 21.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|██▎       | 1.12G/4.98G [00:48<03:46, 17.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|██▎       | 1.13G/4.98G [00:48<03:23, 18.9MB/s]\n",
      "model-00001-of-00004.safetensors:  23%|██▎       | 1.13G/4.98G [00:48<02:37, 24.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  23%|██▎       | 1.14G/4.98G [00:49<03:34, 17.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  23%|██▎       | 1.16G/4.98G [00:49<02:44, 23.2MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  23%|██▎       | 1.16G/4.98G [00:50<02:21, 26.9MB/s]\n",
      "model-00001-of-00004.safetensors:  23%|██▎       | 1.17G/4.98G [00:50<02:07, 29.9MB/s]\n",
      "model-00001-of-00004.safetensors:  24%|██▎       | 1.17G/4.98G [00:50<02:54, 21.8MB/s]\n",
      "model-00001-of-00004.safetensors:  24%|██▎       | 1.18G/4.98G [00:50<02:02, 31.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.18G/4.98G [00:50<02:49, 22.4MB/s]\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.19G/4.98G [00:51<02:17, 27.6MB/s]\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.20G/4.98G [00:51<01:58, 31.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.20G/4.98G [00:51<02:34, 24.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.21G/4.98G [00:51<02:10, 28.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.21G/4.98G [00:51<02:01, 31.1MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.21G/4.98G [00:51<01:51, 33.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|██▍       | 1.22G/4.98G [00:52<02:47, 22.4MB/s]\n",
      "model-00001-of-00004.safetensors:  25%|██▍       | 1.23G/4.98G [00:52<01:36, 38.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|██▍       | 1.24G/4.98G [00:52<02:19, 26.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|██▍       | 1.24G/4.98G [00:52<02:04, 30.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|██▌       | 1.25G/4.98G [00:52<01:51, 33.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  25%|██▌       | 1.25G/4.98G [00:53<02:37, 23.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  25%|██▌       | 1.26G/4.98G [00:53<02:03, 30.0MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  25%|██▌       | 1.26G/4.98G [00:53<01:40, 36.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|██▌       | 1.27G/4.98G [00:53<02:20, 26.4MB/s]\n",
      "model-00001-of-00004.safetensors:  26%|██▌       | 1.28G/4.98G [00:53<01:35, 38.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  26%|██▌       | 1.29G/4.98G [00:54<01:31, 40.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  26%|██▌       | 1.30G/4.98G [00:54<02:10, 28.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|██▋       | 1.31G/4.98G [00:54<01:53, 32.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  26%|██▋       | 1.31G/4.98G [00:55<02:19, 26.2MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  26%|██▋       | 1.32G/4.98G [00:55<02:06, 29.0MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.32G/4.98G [00:55<01:47, 33.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.33G/4.98G [00:55<02:37, 23.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.33G/4.98G [00:56<02:56, 20.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.33G/4.98G [00:56<03:22, 18.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.34G/4.98G [00:57<06:36, 9.19MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.34G/4.98G [00:57<06:54, 8.79MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.34G/4.98G [00:57<06:46, 8.94MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.34G/4.98G [00:57<06:09, 9.85MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.35G/4.98G [00:58<04:04, 14.8MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.36G/4.98G [00:58<03:00, 20.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.36G/4.98G [00:58<03:27, 17.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|██▋       | 1.37G/4.98G [00:58<02:52, 21.0MB/s]\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.37G/4.98G [00:58<02:22, 25.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.38G/4.98G [00:59<02:43, 22.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.38G/4.98G [00:59<02:18, 26.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.39G/4.98G [00:59<02:02, 29.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.39G/4.98G [00:59<02:56, 20.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.40G/4.98G [00:59<02:19, 25.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.40G/4.98G [01:00<01:56, 30.6MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.41G/4.98G [01:00<01:47, 33.3MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.41G/4.98G [01:00<02:34, 23.1MB/s]\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|██▊       | 1.42G/4.98G [01:00<01:48, 32.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|██▊       | 1.42G/4.98G [01:00<01:43, 34.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  29%|██▉       | 1.44G/4.98G [01:01<01:39, 35.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|██████████| 1.17G/1.17G [01:01<00:00, 19.0MB/s]\n",
      "model-00001-of-00004.safetensors:  29%|██▉       | 1.45G/4.98G [01:01<01:38, 35.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|██▉       | 1.47G/4.98G [01:02<01:29, 39.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|██▉       | 1.49G/4.98G [01:02<01:32, 37.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|███       | 1.50G/4.98G [01:03<01:33, 37.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|███       | 1.52G/4.98G [01:03<01:32, 37.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|███       | 1.54G/4.98G [01:04<02:12, 26.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|███       | 1.55G/4.98G [01:05<01:58, 29.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|███▏      | 1.56G/4.98G [01:05<02:32, 22.4MB/s]\n",
      "model-00001-of-00004.safetensors:  31%|███▏      | 1.56G/4.98G [01:05<02:07, 26.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|███▏      | 1.57G/4.98G [01:06<02:46, 20.5MB/s]\n",
      "model-00001-of-00004.safetensors:  32%|███▏      | 1.58G/4.98G [01:06<02:16, 24.8MB/s]\n",
      "model-00001-of-00004.safetensors:  32%|███▏      | 1.58G/4.98G [01:06<01:55, 29.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|███▏      | 1.59G/4.98G [01:06<02:30, 22.6MB/s]\n",
      "model-00001-of-00004.safetensors:  32%|███▏      | 1.59G/4.98G [01:06<02:01, 27.9MB/s]\n",
      "model-00001-of-00004.safetensors:  32%|███▏      | 1.60G/4.98G [01:07<01:41, 33.1MB/s]\n",
      "model-00001-of-00004.safetensors:  32%|███▏      | 1.61G/4.98G [01:07<01:31, 36.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|███▎      | 1.63G/4.98G [01:08<01:32, 36.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|███▎      | 1.64G/4.98G [01:08<01:38, 33.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|███▎      | 1.66G/4.98G [01:09<01:34, 35.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|███▎      | 1.68G/4.98G [01:09<01:34, 34.9MB/s]\n",
      "model-00001-of-00004.safetensors:  34%|███▍      | 1.68G/4.98G [01:10<01:57, 28.1MB/s]\n",
      "model-00001-of-00004.safetensors:  34%|███▍      | 1.69G/4.98G [01:10<01:43, 31.8MB/s]\n",
      "model-00001-of-00004.safetensors:  34%|███▍      | 1.70G/4.98G [01:10<01:31, 35.9MB/s]\n",
      "model-00001-of-00004.safetensors:  34%|███▍      | 1.70G/4.98G [01:10<02:07, 25.7MB/s]\n",
      "model-00001-of-00004.safetensors:  34%|███▍      | 1.71G/4.98G [01:10<01:38, 33.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|███▍      | 1.72G/4.98G [01:11<01:29, 36.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|███▍      | 1.74G/4.98G [01:11<01:33, 34.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|███▌      | 1.76G/4.98G [01:12<01:29, 35.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|███▌      | 1.77G/4.98G [01:12<01:26, 37.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|███▌      | 1.79G/4.98G [01:13<01:23, 38.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|███▋      | 1.81G/4.98G [01:14<01:58, 26.8MB/s]\n",
      "model-00001-of-00004.safetensors:  36%|███▋      | 1.81G/4.98G [01:14<01:48, 29.1MB/s]\n",
      "model-00001-of-00004.safetensors:  37%|███▋      | 1.82G/4.98G [01:14<01:35, 33.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|███▋      | 1.82G/4.98G [01:14<02:27, 21.3MB/s]\n",
      "model-00001-of-00004.safetensors:  37%|███▋      | 1.84G/4.98G [01:15<01:32, 34.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|███▋      | 1.86G/4.98G [01:15<01:22, 37.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|███▊      | 1.87G/4.98G [01:16<01:28, 35.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|███▊      | 1.88G/4.98G [01:16<02:10, 23.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|███▊      | 1.90G/4.98G [01:17<01:42, 30.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|███▊      | 1.92G/4.98G [01:17<01:29, 34.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|███▉      | 1.93G/4.98G [01:18<01:23, 36.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|███▉      | 1.95G/4.98G [01:18<01:25, 35.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|███▉      | 1.96G/4.98G [01:19<01:28, 33.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|███▉      | 1.98G/4.98G [01:19<01:25, 35.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|████      | 1.99G/4.98G [01:20<01:23, 35.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|████      | 2.01G/4.98G [01:20<01:15, 39.0MB/s]\n",
      "model-00001-of-00004.safetensors:  41%|████      | 2.02G/4.98G [01:21<01:41, 29.1MB/s]\n",
      "model-00001-of-00004.safetensors:  41%|████      | 2.03G/4.98G [01:21<01:23, 35.5MB/s]\n",
      "model-00001-of-00004.safetensors:  41%|████      | 2.03G/4.98G [01:21<01:40, 29.3MB/s]\n",
      "model-00001-of-00004.safetensors:  41%|████      | 2.04G/4.98G [01:21<01:24, 35.0MB/s]\n",
      "model-00001-of-00004.safetensors:  41%|████      | 2.05G/4.98G [01:21<01:10, 41.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|████▏     | 2.08G/4.98G [01:22<01:10, 40.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|████▏     | 2.09G/4.98G [01:23<01:10, 40.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|████▏     | 2.11G/4.98G [01:23<01:13, 39.1MB/s]\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.12G/4.98G [01:24<01:30, 31.8MB/s]\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.12G/4.98G [01:24<01:18, 36.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.13G/4.98G [01:24<01:49, 26.1MB/s]\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.13G/4.98G [01:24<01:31, 31.1MB/s]\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.14G/4.98G [01:24<01:18, 36.3MB/s]\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.15G/4.98G [01:25<01:47, 26.4MB/s]\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.15G/4.98G [01:25<01:24, 33.5MB/s]\n",
      "model-00001-of-00004.safetensors:  43%|████▎     | 2.16G/4.98G [01:25<01:18, 36.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|████▎     | 2.17G/4.98G [01:26<01:46, 26.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|████▍     | 2.19G/4.98G [01:26<01:26, 32.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|████▍     | 2.20G/4.98G [01:27<01:24, 33.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|████▍     | 2.21G/4.98G [01:27<01:59, 23.2MB/s]\n",
      "model-00001-of-00004.safetensors:  45%|████▍     | 2.22G/4.98G [01:27<01:40, 27.5MB/s]\n",
      "model-00001-of-00004.safetensors:  45%|████▍     | 2.22G/4.98G [01:28<01:25, 32.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|████▍     | 2.23G/4.98G [01:28<01:51, 24.7MB/s]\n",
      "model-00001-of-00004.safetensors:  45%|████▍     | 2.23G/4.98G [01:28<01:33, 29.3MB/s]\n",
      "model-00001-of-00004.safetensors:  45%|████▍     | 2.24G/4.98G [01:28<01:20, 34.0MB/s]\n",
      "model-00001-of-00004.safetensors:  45%|████▌     | 2.25G/4.98G [01:28<01:08, 39.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|████▌     | 2.27G/4.98G [01:29<01:08, 39.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|████▌     | 2.29G/4.98G [01:29<01:09, 38.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|████▌     | 2.30G/4.98G [01:30<01:17, 34.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|████▋     | 2.32G/4.98G [01:30<01:13, 36.2MB/s]\n",
      "model-00001-of-00004.safetensors:  47%|████▋     | 2.33G/4.98G [01:31<01:15, 34.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|████▋     | 2.34G/4.98G [01:32<01:29, 29.5MB/s]\n",
      "model-00001-of-00004.safetensors:  47%|████▋     | 2.35G/4.98G [01:32<01:17, 33.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|████▋     | 2.35G/4.98G [01:32<01:51, 23.4MB/s]\n",
      "model-00001-of-00004.safetensors:  47%|████▋     | 2.36G/4.98G [01:32<01:21, 32.1MB/s]\n",
      "model-00001-of-00004.safetensors:  48%|████▊     | 2.37G/4.98G [01:33<01:48, 24.1MB/s]\n",
      "model-00001-of-00004.safetensors:  48%|████▊     | 2.38G/4.98G [01:33<01:24, 30.7MB/s]\n",
      "model-00001-of-00004.safetensors:  48%|████▊     | 2.38G/4.98G [01:33<01:22, 31.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|████▊     | 2.40G/4.98G [01:33<01:09, 37.1MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|████▊     | 2.41G/4.98G [01:34<01:08, 37.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|████▊     | 2.42G/4.98G [01:34<01:25, 29.9MB/s]\n",
      "model-00001-of-00004.safetensors:  49%|████▊     | 2.43G/4.98G [01:34<01:16, 33.5MB/s]\n",
      "model-00001-of-00004.safetensors:  49%|████▉     | 2.43G/4.98G [01:34<01:34, 27.0MB/s]\n",
      "model-00001-of-00004.safetensors:  49%|████▉     | 2.44G/4.98G [01:35<01:22, 30.8MB/s]\n",
      "model-00001-of-00004.safetensors:  49%|████▉     | 2.44G/4.98G [01:35<01:13, 34.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|████▉     | 2.46G/4.98G [01:35<01:13, 34.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|████▉     | 2.47G/4.98G [01:36<01:21, 30.9MB/s]\n",
      "model-00001-of-00004.safetensors:  50%|████▉     | 2.48G/4.98G [01:36<01:06, 37.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█████     | 2.49G/4.98G [01:36<01:01, 40.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█████     | 2.51G/4.98G [01:37<01:28, 27.9MB/s]\n",
      "model-00001-of-00004.safetensors:  51%|█████     | 2.52G/4.98G [01:37<01:23, 29.5MB/s]\n",
      "model-00001-of-00004.safetensors:  51%|█████     | 2.52G/4.98G [01:37<01:18, 31.3MB/s]\n",
      "model-00001-of-00004.safetensors:  51%|█████     | 2.52G/4.98G [01:37<01:15, 32.3MB/s]\n",
      "model-00001-of-00004.safetensors:  51%|█████     | 2.53G/4.98G [01:38<01:52, 21.7MB/s]\n",
      "model-00001-of-00004.safetensors:  51%|█████     | 2.53G/4.98G [01:38<01:46, 23.0MB/s]\n",
      "model-00001-of-00004.safetensors:  51%|█████     | 2.53G/4.98G [01:38<01:41, 24.0MB/s]\n",
      "model-00001-of-00004.safetensors:  51%|█████     | 2.54G/4.98G [01:38<01:39, 24.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|█████▏    | 2.55G/4.98G [01:39<01:24, 28.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|█████▏    | 2.57G/4.98G [01:39<01:09, 34.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|█████▏    | 2.59G/4.98G [01:40<01:14, 32.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|█████▏    | 2.59G/4.98G [01:40<01:41, 23.5MB/s]\n",
      "model-00001-of-00004.safetensors:  52%|█████▏    | 2.60G/4.98G [01:40<01:30, 26.4MB/s]\n",
      "model-00001-of-00004.safetensors:  52%|█████▏    | 2.60G/4.98G [01:41<01:22, 28.7MB/s]\n",
      "model-00001-of-00004.safetensors:  52%|█████▏    | 2.61G/4.98G [01:41<01:15, 31.3MB/s]\n",
      "model-00001-of-00004.safetensors:  52%|█████▏    | 2.61G/4.98G [01:41<01:40, 23.6MB/s]\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.61G/4.98G [01:41<01:25, 27.7MB/s]\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.62G/4.98G [01:41<01:12, 32.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.63G/4.98G [01:42<02:00, 19.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.63G/4.98G [01:42<02:54, 13.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.64G/4.98G [01:43<03:12, 12.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.65G/4.98G [01:43<02:36, 14.9MB/s]\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.65G/4.98G [01:43<02:02, 19.1MB/s]\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.65G/4.98G [01:44<01:51, 20.9MB/s]\n",
      "model-00001-of-00004.safetensors:  53%|█████▎    | 2.66G/4.98G [01:44<01:44, 22.2MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▎    | 2.67G/4.98G [01:44<01:28, 26.1MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▎    | 2.67G/4.98G [01:44<01:15, 30.7MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▎    | 2.67G/4.98G [01:44<02:04, 18.5MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▍    | 2.68G/4.98G [01:45<01:41, 22.5MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▍    | 2.68G/4.98G [01:45<01:26, 26.4MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▍    | 2.69G/4.98G [01:45<01:15, 30.4MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▍    | 2.69G/4.98G [01:45<01:49, 20.9MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▍    | 2.70G/4.98G [01:45<01:27, 26.2MB/s]\n",
      "model-00001-of-00004.safetensors:  54%|█████▍    | 2.71G/4.98G [01:46<01:41, 22.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|█████▍    | 2.71G/4.98G [01:46<01:24, 26.9MB/s]\n",
      "model-00001-of-00004.safetensors:  55%|█████▍    | 2.72G/4.98G [01:46<01:05, 34.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|█████▍    | 2.72G/4.98G [01:46<01:51, 20.2MB/s]\n",
      "model-00001-of-00004.safetensors:  55%|█████▍    | 2.73G/4.98G [01:47<01:30, 24.8MB/s]\n",
      "model-00001-of-00004.safetensors:  55%|█████▍    | 2.73G/4.98G [01:47<01:15, 29.7MB/s]\n",
      "model-00001-of-00004.safetensors:  55%|█████▌    | 2.74G/4.98G [01:47<01:45, 21.3MB/s]\n",
      "model-00001-of-00004.safetensors:  55%|█████▌    | 2.74G/4.98G [01:47<01:19, 28.2MB/s]\n",
      "model-00001-of-00004.safetensors:  55%|█████▌    | 2.75G/4.98G [01:47<01:05, 34.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|█████▌    | 2.76G/4.98G [01:48<01:11, 30.8MB/s]\n",
      "model-00001-of-00004.safetensors:  56%|█████▌    | 2.77G/4.98G [01:48<01:34, 23.4MB/s]\n",
      "model-00001-of-00004.safetensors:  56%|█████▌    | 2.77G/4.98G [01:48<01:16, 28.8MB/s]\n",
      "model-00001-of-00004.safetensors:  56%|█████▌    | 2.78G/4.98G [01:48<01:04, 33.9MB/s]\n",
      "model-00001-of-00004.safetensors:  56%|█████▌    | 2.79G/4.98G [01:49<01:33, 23.4MB/s]\n",
      "model-00001-of-00004.safetensors:  56%|█████▌    | 2.79G/4.98G [01:49<01:15, 28.8MB/s]\n",
      "model-00001-of-00004.safetensors:  56%|█████▌    | 2.80G/4.98G [01:49<01:11, 30.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|█████▋    | 2.80G/4.98G [01:49<01:40, 21.6MB/s]\n",
      "model-00001-of-00004.safetensors:  56%|█████▋    | 2.81G/4.98G [01:49<01:11, 30.2MB/s]\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.81G/4.98G [01:49<01:02, 34.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.82G/4.98G [01:50<01:39, 21.6MB/s]\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.82G/4.98G [01:50<01:21, 26.3MB/s]\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.83G/4.98G [01:50<01:14, 28.9MB/s]\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.83G/4.98G [01:50<01:08, 31.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.84G/4.98G [01:51<01:15, 28.2MB/s]\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.85G/4.98G [01:51<01:08, 31.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|█████▋    | 2.86G/4.98G [01:51<00:59, 35.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|█████▊    | 2.88G/4.98G [01:52<01:18, 26.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|█████▊    | 2.89G/4.98G [01:53<01:29, 23.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|█████▊    | 2.90G/4.98G [01:53<01:28, 23.4MB/s]\n",
      "model-00001-of-00004.safetensors:  58%|█████▊    | 2.91G/4.98G [01:53<00:59, 34.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|█████▉    | 2.93G/4.98G [01:54<01:42, 20.0MB/s]\n",
      "model-00001-of-00004.safetensors:  59%|█████▉    | 2.93G/4.98G [01:54<01:24, 24.3MB/s]\n",
      "model-00001-of-00004.safetensors:  59%|█████▉    | 2.94G/4.98G [01:55<01:10, 29.0MB/s]\n",
      "model-00001-of-00004.safetensors:  59%|█████▉    | 2.94G/4.98G [01:55<01:28, 22.9MB/s]\n",
      "model-00001-of-00004.safetensors:  59%|█████▉    | 2.95G/4.98G [01:55<01:06, 30.4MB/s]\n",
      "model-00001-of-00004.safetensors:  59%|█████▉    | 2.96G/4.98G [01:55<00:58, 34.5MB/s]\n",
      "model-00001-of-00004.safetensors:  62%|██████▏   | 3.11G/4.98G [02:00<00:59, 31.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██████▎   | 3.13G/4.98G [02:01<00:50, 36.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██████▎   | 3.15G/4.98G [02:01<00:48, 37.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██████▎   | 3.16G/4.98G [02:02<00:58, 31.3MB/s]\n",
      "model-00001-of-00004.safetensors:  64%|██████▎   | 3.16G/4.98G [02:02<00:55, 32.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.17G/4.98G [02:02<00:59, 30.3MB/s]\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.18G/4.98G [02:03<00:54, 32.7MB/s]\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.18G/4.98G [02:03<00:52, 33.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.19G/4.98G [02:04<01:50, 16.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.20G/4.98G [02:05<03:07, 9.50MB/s]\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.20G/4.98G [02:05<02:53, 10.3MB/s]\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.20G/4.98G [02:05<04:42, 6.29MB/s]\n",
      "model-00001-of-00004.safetensors:  64%|██████▍   | 3.21G/4.98G [02:06<02:25, 12.2MB/s]\n",
      "model-00001-of-00004.safetensors:  65%|██████▍   | 3.21G/4.98G [02:06<01:50, 16.0MB/s]\n",
      "model-00001-of-00004.safetensors:  65%|██████▍   | 3.22G/4.98G [02:06<01:22, 21.4MB/s]\n",
      "model-00001-of-00004.safetensors:  65%|██████▍   | 3.22G/4.98G [02:06<01:45, 16.6MB/s]\n",
      "model-00001-of-00004.safetensors:  65%|██████▍   | 3.23G/4.98G [02:06<01:13, 24.0MB/s]\n",
      "model-00001-of-00004.safetensors:  65%|██████▍   | 3.23G/4.98G [02:06<01:04, 27.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██████▌   | 3.24G/4.98G [02:07<00:50, 34.7MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██████▌   | 3.26G/4.98G [02:07<00:47, 36.5MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▌   | 3.27G/4.98G [02:07<00:59, 29.0MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▌   | 3.27G/4.98G [02:08<00:51, 33.1MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▌   | 3.28G/4.98G [02:08<00:44, 38.2MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▌   | 3.28G/4.98G [02:08<00:54, 31.1MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▌   | 3.29G/4.98G [02:08<00:52, 32.0MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▌   | 3.29G/4.98G [02:08<00:45, 36.9MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▌   | 3.30G/4.98G [02:08<01:02, 26.7MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▋   | 3.30G/4.98G [02:09<00:54, 31.0MB/s]\n",
      "model-00001-of-00004.safetensors:  66%|██████▋   | 3.31G/4.98G [02:09<00:48, 34.6MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.31G/4.98G [02:09<00:43, 38.0MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.32G/4.98G [02:09<01:06, 25.0MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.32G/4.98G [02:09<00:56, 29.2MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.33G/4.98G [02:09<00:48, 34.1MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.33G/4.98G [02:10<01:11, 22.9MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.33G/4.98G [02:10<01:04, 25.5MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.34G/4.98G [02:10<00:59, 27.4MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.34G/4.98G [02:10<01:13, 22.3MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.35G/4.98G [02:10<00:58, 27.6MB/s]\n",
      "model-00001-of-00004.safetensors:  67%|██████▋   | 3.35G/4.98G [02:10<00:51, 31.7MB/s]\n",
      "model-00001-of-00004.safetensors:  68%|██████▊   | 3.36G/4.98G [02:11<01:04, 24.9MB/s]\n",
      "model-00001-of-00004.safetensors:  68%|██████▊   | 3.37G/4.98G [02:11<00:51, 31.4MB/s]\n",
      "model-00001-of-00004.safetensors:  68%|██████▊   | 3.38G/4.98G [02:11<00:57, 28.0MB/s]\n",
      "model-00001-of-00004.safetensors:  68%|██████▊   | 3.39G/4.98G [02:11<00:36, 43.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██████▊   | 3.41G/4.98G [02:12<00:40, 39.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██████▊   | 3.41G/4.98G [02:12<00:56, 27.9MB/s]\n",
      "model-00001-of-00004.safetensors:  69%|██████▊   | 3.42G/4.98G [02:12<00:43, 35.4MB/s]\n",
      "model-00001-of-00004.safetensors:  69%|██████▉   | 3.43G/4.98G [02:13<00:55, 27.7MB/s]\n",
      "model-00001-of-00004.safetensors:  69%|██████▉   | 3.43G/4.98G [02:13<00:53, 29.0MB/s]\n",
      "model-00001-of-00004.safetensors:  69%|██████▉   | 3.44G/4.98G [02:13<00:50, 30.5MB/s]\n",
      "model-00001-of-00004.safetensors:  69%|██████▉   | 3.44G/4.98G [02:13<01:00, 25.4MB/s]\n",
      "model-00001-of-00004.safetensors:  69%|██████▉   | 3.45G/4.98G [02:14<00:42, 35.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██████▉   | 3.47G/4.98G [02:14<00:38, 39.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|███████   | 3.49G/4.98G [02:14<00:34, 43.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|███████   | 3.50G/4.98G [02:15<00:45, 32.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|███████   | 3.51G/4.98G [02:16<00:42, 34.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|███████   | 3.53G/4.98G [02:16<00:38, 37.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|███████▏  | 3.55G/4.98G [02:17<00:44, 31.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███████▏  | 3.57G/4.98G [02:17<00:39, 35.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███████▏  | 3.57G/4.98G [02:18<00:50, 27.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███████▏  | 3.59G/4.98G [02:18<00:42, 32.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███████▎  | 3.61G/4.98G [02:19<00:37, 36.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███████▎  | 3.63G/4.98G [02:20<00:41, 32.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███████▎  | 3.64G/4.98G [02:20<00:40, 32.6MB/s]\n",
      "model-00001-of-00004.safetensors:  73%|███████▎  | 3.65G/4.98G [02:20<00:56, 23.6MB/s]\n",
      "model-00001-of-00004.safetensors:  73%|███████▎  | 3.65G/4.98G [02:20<00:49, 26.9MB/s]\n",
      "model-00001-of-00004.safetensors:  74%|███████▎  | 3.66G/4.98G [02:21<00:45, 28.9MB/s]\n",
      "model-00001-of-00004.safetensors:  74%|███████▎  | 3.66G/4.98G [02:21<01:01, 21.4MB/s]\n",
      "model-00001-of-00004.safetensors:  74%|███████▍  | 3.67G/4.98G [02:21<00:47, 27.3MB/s]\n",
      "model-00001-of-00004.safetensors:  74%|███████▍  | 3.69G/4.98G [02:22<00:43, 29.7MB/s]\n",
      "model-00001-of-00004.safetensors:  74%|███████▍  | 3.69G/4.98G [02:22<00:39, 32.3MB/s]\n",
      "model-00001-of-00004.safetensors:  74%|███████▍  | 3.71G/4.98G [02:22<00:39, 31.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███████▍  | 3.72G/4.98G [02:23<00:42, 29.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███████▍  | 3.73G/4.98G [02:23<00:51, 24.3MB/s]\n",
      "model-00001-of-00004.safetensors:  75%|███████▌  | 3.74G/4.98G [02:24<00:38, 31.9MB/s]\n",
      "model-00001-of-00004.safetensors:  75%|███████▌  | 3.74G/4.98G [02:24<00:34, 35.3MB/s]\n",
      "model-00001-of-00004.safetensors:  76%|███████▌  | 3.76G/4.98G [02:24<00:31, 39.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███████▌  | 3.76G/4.98G [02:24<00:45, 26.7MB/s]\n",
      "model-00001-of-00004.safetensors:  76%|███████▌  | 3.77G/4.98G [02:25<00:40, 30.0MB/s]\n",
      "model-00001-of-00004.safetensors:  76%|███████▌  | 3.77G/4.98G [02:25<00:35, 33.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███████▌  | 3.79G/4.98G [02:25<00:42, 28.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███████▋  | 3.80G/4.98G [02:26<00:36, 32.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███████▋  | 3.81G/4.98G [02:26<00:44, 25.9MB/s]\n",
      "model-00001-of-00004.safetensors:  77%|███████▋  | 3.82G/4.98G [02:27<00:34, 33.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███████▋  | 3.84G/4.98G [02:27<00:35, 32.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███████▋  | 3.85G/4.98G [02:28<00:50, 22.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███████▊  | 3.87G/4.98G [02:29<00:33, 33.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███████▊  | 3.88G/4.98G [02:29<00:41, 26.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███████▊  | 3.90G/4.98G [02:30<00:27, 39.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███████▊  | 3.91G/4.98G [02:30<00:33, 31.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███████▉  | 3.92G/4.98G [02:32<02:16, 7.76MB/s]\n",
      "model-00001-of-00004.safetensors:  79%|███████▉  | 3.93G/4.98G [02:32<01:29, 11.7MB/s]\n",
      "model-00001-of-00004.safetensors:  79%|███████▉  | 3.93G/4.98G [02:33<01:15, 13.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███████▉  | 3.95G/4.98G [02:33<00:46, 22.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███████▉  | 3.97G/4.98G [02:34<00:34, 29.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███████▉  | 3.98G/4.98G [02:34<00:37, 26.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|████████  | 3.98G/4.98G [02:35<01:01, 16.0MB/s]\n",
      "model-00001-of-00004.safetensors:  80%|████████  | 4.00G/4.98G [02:35<00:40, 24.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|████████  | 4.01G/4.98G [02:36<00:33, 28.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|████████  | 4.02G/4.98G [02:36<00:44, 21.7MB/s]\n",
      "model-00001-of-00004.safetensors:  81%|████████  | 4.03G/4.98G [02:36<00:33, 28.0MB/s]\n",
      "model-00001-of-00004.safetensors:  81%|████████  | 4.03G/4.98G [02:36<00:29, 32.2MB/s]\n",
      "model-00001-of-00004.safetensors:  81%|████████▏ | 4.05G/4.98G [02:37<00:26, 35.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████████▏ | 4.06G/4.98G [02:37<00:26, 34.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████████▏ | 4.07G/4.98G [02:38<00:24, 36.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████████▏ | 4.09G/4.98G [02:38<00:24, 36.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████████▏ | 4.10G/4.98G [02:39<00:26, 32.8MB/s]\n",
      "model-00001-of-00004.safetensors:  83%|████████▎ | 4.11G/4.98G [02:39<00:20, 43.1MB/s]\n",
      "model-00001-of-00004.safetensors:  83%|████████▎ | 4.12G/4.98G [02:39<00:22, 37.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|████████▎ | 4.14G/4.98G [02:40<00:24, 34.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|████████▎ | 4.15G/4.98G [02:40<00:20, 39.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████████▍ | 4.18G/4.98G [02:41<00:20, 39.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████████▍ | 4.19G/4.98G [02:41<00:18, 42.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████████▍ | 4.20G/4.98G [02:42<00:23, 33.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|████████▍ | 4.22G/4.98G [02:42<00:21, 35.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|████████▌ | 4.23G/4.98G [02:43<00:20, 36.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████████▌ | 4.26G/4.98G [02:43<00:19, 37.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████████▌ | 4.27G/4.98G [02:44<00:17, 40.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████████▌ | 4.28G/4.98G [02:44<00:24, 28.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████████▋ | 4.30G/4.98G [02:45<00:20, 33.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████████▋ | 4.32G/4.98G [02:46<00:21, 30.3MB/s]\n",
      "model-00001-of-00004.safetensors:  87%|████████▋ | 4.33G/4.98G [02:46<00:15, 41.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████████▋ | 4.35G/4.98G [02:47<00:26, 23.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|████████▊ | 4.37G/4.98G [02:47<00:19, 32.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|████████▊ | 4.38G/4.98G [02:48<00:16, 35.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|████████▊ | 4.40G/4.98G [02:48<00:15, 36.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|████████▊ | 4.41G/4.98G [02:49<00:17, 33.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|████████▊ | 4.42G/4.98G [02:49<00:20, 27.3MB/s]\n",
      "model-00001-of-00004.safetensors:  89%|████████▉ | 4.42G/4.98G [02:49<00:16, 33.3MB/s]\n",
      "model-00001-of-00004.safetensors:  89%|████████▉ | 4.43G/4.98G [02:49<00:14, 38.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|████████▉ | 4.44G/4.98G [02:51<00:48, 11.1MB/s]\n",
      "model-00001-of-00004.safetensors:  89%|████████▉ | 4.45G/4.98G [02:52<00:36, 14.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|████████▉ | 4.46G/4.98G [02:52<00:29, 17.9MB/s]\n",
      "model-00001-of-00004.safetensors:  90%|████████▉ | 4.46G/4.98G [02:52<00:25, 20.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|████████▉ | 4.47G/4.98G [02:53<00:33, 15.2MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|████████▉ | 4.47G/4.98G [02:53<00:31, 16.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|████████▉ | 4.48G/4.98G [02:53<00:33, 15.1MB/s]\n",
      "model-00001-of-00004.safetensors:  90%|████████▉ | 4.48G/4.98G [02:53<00:32, 15.2MB/s]\n",
      "model-00001-of-00004.safetensors:  90%|█████████ | 4.48G/4.98G [02:53<00:30, 16.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|█████████ | 4.49G/4.98G [02:54<00:27, 18.1MB/s]\n",
      "model-00001-of-00004.safetensors:  90%|█████████ | 4.49G/4.98G [02:54<00:27, 17.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|█████████ | 4.49G/4.98G [02:54<00:28, 16.8MB/s]\n",
      "model-00001-of-00004.safetensors:  90%|█████████ | 4.49G/4.98G [02:54<00:27, 17.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|█████████ | 4.50G/4.98G [02:55<00:29, 16.0MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.51G/4.98G [02:55<00:28, 16.6MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.51G/4.98G [02:55<00:27, 16.9MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.51G/4.98G [02:55<00:26, 17.5MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.51G/4.98G [02:56<00:27, 17.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.51G/4.98G [02:56<00:48, 9.56MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.52G/4.98G [02:56<00:37, 12.3MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.52G/4.98G [02:56<00:35, 12.8MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.52G/4.98G [02:56<00:31, 14.7MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.52G/4.98G [02:57<00:27, 16.3MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.52G/4.98G [02:57<00:26, 16.9MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.53G/4.98G [02:57<00:27, 16.3MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.53G/4.98G [02:57<00:49, 8.98MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.53G/4.98G [02:57<00:41, 10.7MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.53G/4.98G [02:57<00:38, 11.5MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.53G/4.98G [02:57<00:31, 14.0MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████ | 4.54G/4.98G [02:58<00:25, 17.4MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████▏| 4.54G/4.98G [02:58<00:18, 23.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|█████████▏| 4.55G/4.98G [02:58<00:27, 15.5MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████▏| 4.55G/4.98G [02:58<00:26, 15.9MB/s]\n",
      "model-00001-of-00004.safetensors:  91%|█████████▏| 4.55G/4.98G [02:59<00:26, 16.3MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.55G/4.98G [02:59<00:25, 16.6MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.56G/4.98G [02:59<00:25, 16.5MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.56G/4.98G [02:59<00:24, 16.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.57G/4.98G [03:00<00:28, 14.3MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.57G/4.98G [03:00<00:27, 14.9MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.57G/4.98G [03:00<00:28, 14.4MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.57G/4.98G [03:00<00:26, 15.1MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.57G/4.98G [03:00<00:23, 17.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.58G/4.98G [03:00<00:33, 12.1MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.58G/4.98G [03:00<00:25, 15.4MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.58G/4.98G [03:01<00:24, 16.4MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.59G/4.98G [03:01<00:19, 19.4MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.59G/4.98G [03:01<00:20, 19.1MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.59G/4.98G [03:01<00:28, 13.6MB/s]\n",
      "model-00001-of-00004.safetensors:  92%|█████████▏| 4.60G/4.98G [03:01<00:24, 15.7MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.60G/4.98G [03:02<00:16, 22.6MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.61G/4.98G [03:02<00:18, 20.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.61G/4.98G [03:02<00:28, 12.7MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.61G/4.98G [03:02<00:23, 15.5MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.61G/4.98G [03:02<00:19, 18.3MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.62G/4.98G [03:03<00:17, 20.7MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.62G/4.98G [03:03<00:16, 22.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.62G/4.98G [03:03<00:27, 12.8MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.63G/4.98G [03:03<00:24, 14.4MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.63G/4.98G [03:03<00:22, 15.1MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.63G/4.98G [03:03<00:22, 15.7MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.63G/4.98G [03:04<00:21, 15.9MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.63G/4.98G [03:04<00:20, 16.4MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.64G/4.98G [03:04<00:18, 17.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.64G/4.98G [03:04<00:29, 11.2MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.64G/4.98G [03:04<00:22, 14.9MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.65G/4.98G [03:04<00:18, 17.9MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.65G/4.98G [03:05<00:16, 19.3MB/s]\n",
      "model-00001-of-00004.safetensors:  93%|█████████▎| 4.65G/4.98G [03:05<00:15, 20.9MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▎| 4.65G/4.98G [03:05<00:14, 21.6MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▎| 4.66G/4.98G [03:05<00:24, 13.0MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▎| 4.66G/4.98G [03:05<00:22, 14.2MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▎| 4.66G/4.98G [03:05<00:19, 15.8MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▎| 4.66G/4.98G [03:05<00:17, 17.4MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.67G/4.98G [03:06<00:17, 18.0MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.67G/4.98G [03:06<00:16, 18.5MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.67G/4.98G [03:06<00:16, 18.9MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.67G/4.98G [03:06<00:16, 18.9MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.67G/4.98G [03:06<00:27, 10.9MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.68G/4.98G [03:06<00:22, 13.5MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.68G/4.98G [03:06<00:16, 18.4MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.68G/4.98G [03:07<00:13, 22.5MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.69G/4.98G [03:07<00:11, 24.9MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.69G/4.98G [03:07<00:16, 17.5MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.69G/4.98G [03:07<00:14, 19.8MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.70G/4.98G [03:07<00:13, 21.5MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.70G/4.98G [03:07<00:11, 24.9MB/s]\n",
      "model-00001-of-00004.safetensors:  94%|█████████▍| 4.70G/4.98G [03:07<00:09, 27.7MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▍| 4.71G/4.98G [03:08<00:13, 19.8MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▍| 4.71G/4.98G [03:08<00:12, 21.5MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▍| 4.71G/4.98G [03:08<00:12, 21.7MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▍| 4.72G/4.98G [03:08<00:11, 22.0MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▍| 4.72G/4.98G [03:08<00:11, 23.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|█████████▍| 4.72G/4.98G [03:09<00:18, 14.1MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.73G/4.98G [03:09<00:13, 18.4MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.73G/4.98G [03:09<00:12, 19.6MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.73G/4.98G [03:09<00:12, 20.3MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.74G/4.98G [03:09<00:11, 21.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.74G/4.98G [03:09<00:18, 13.2MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.74G/4.98G [03:10<00:15, 15.4MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.74G/4.98G [03:10<00:15, 15.6MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.75G/4.98G [03:10<00:14, 15.6MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.75G/4.98G [03:10<00:15, 15.0MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.75G/4.98G [03:10<00:15, 14.3MB/s]\n",
      "model-00001-of-00004.safetensors:  95%|█████████▌| 4.75G/4.98G [03:10<00:14, 15.7MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.75G/4.98G [03:11<00:22, 10.1MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.76G/4.98G [03:11<00:18, 12.3MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.77G/4.98G [03:11<00:09, 22.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.77G/4.98G [03:12<00:16, 12.9MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.77G/4.98G [03:12<00:14, 14.6MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.78G/4.98G [03:12<00:11, 18.3MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.78G/4.98G [03:12<00:10, 19.2MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.78G/4.98G [03:12<00:09, 21.7MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.78G/4.98G [03:12<00:12, 15.1MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.79G/4.98G [03:12<00:10, 18.8MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▌| 4.79G/4.98G [03:13<00:09, 20.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|█████████▋| 4.79G/4.98G [03:13<00:09, 19.1MB/s]\n",
      "model-00001-of-00004.safetensors:  96%|█████████▋| 4.80G/4.98G [03:13<00:08, 21.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.81G/4.98G [03:13<00:10, 16.9MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.81G/4.98G [03:14<00:09, 17.3MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.81G/4.98G [03:14<00:09, 17.9MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.81G/4.98G [03:14<00:07, 21.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.82G/4.98G [03:15<00:12, 12.9MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.82G/4.98G [03:15<00:10, 14.3MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.82G/4.98G [03:15<00:09, 15.5MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.83G/4.98G [03:15<00:09, 15.3MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.83G/4.98G [03:15<00:09, 15.9MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.83G/4.98G [03:15<00:08, 16.2MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.83G/4.98G [03:16<00:13, 10.8MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.83G/4.98G [03:16<00:11, 12.0MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.84G/4.98G [03:16<00:10, 13.1MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.84G/4.98G [03:16<00:10, 13.7MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.84G/4.98G [03:16<00:09, 14.3MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.84G/4.98G [03:16<00:09, 14.8MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.84G/4.98G [03:16<00:08, 15.9MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.85G/4.98G [03:16<00:08, 15.0MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.85G/4.98G [03:16<00:08, 14.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.85G/4.98G [03:17<00:13, 9.35MB/s]\n",
      "model-00001-of-00004.safetensors:  97%|█████████▋| 4.85G/4.98G [03:17<00:12, 10.4MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.85G/4.98G [03:17<00:10, 11.8MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.86G/4.98G [03:17<00:08, 14.2MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.86G/4.98G [03:17<00:07, 15.7MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.86G/4.98G [03:17<00:07, 15.1MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.86G/4.98G [03:18<00:07, 15.6MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.86G/4.98G [03:18<00:07, 15.7MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.87G/4.98G [03:18<00:06, 15.9MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.87G/4.98G [03:18<00:06, 17.5MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.87G/4.98G [03:18<00:05, 18.5MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.88G/4.98G [03:18<00:04, 20.8MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.88G/4.98G [03:19<00:04, 19.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.88G/4.98G [03:19<00:08, 11.6MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.89G/4.98G [03:19<00:04, 18.3MB/s]\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.89G/4.98G [03:19<00:04, 17.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.89G/4.98G [03:20<00:04, 16.7MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|█████████▊| 4.90G/4.98G [03:20<00:07, 10.8MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▊| 4.91G/4.98G [03:21<00:02, 22.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|█████████▊| 4.91G/4.98G [03:21<00:04, 15.5MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.92G/4.98G [03:21<00:03, 17.8MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.92G/4.98G [03:21<00:03, 18.2MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.92G/4.98G [03:21<00:02, 20.3MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.93G/4.98G [03:21<00:02, 24.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.93G/4.98G [03:22<00:03, 13.2MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.93G/4.98G [03:22<00:02, 15.6MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.94G/4.98G [03:22<00:02, 20.3MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.94G/4.98G [03:22<00:01, 26.2MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.94G/4.98G [03:22<00:01, 17.8MB/s]\n",
      "model-00001-of-00004.safetensors:  99%|█████████▉| 4.95G/4.98G [03:23<00:01, 22.7MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|█████████▉| 4.95G/4.98G [03:23<00:00, 27.0MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|█████████▉| 4.96G/4.98G [03:23<00:00, 29.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|█████████▉| 4.97G/4.98G [03:23<00:00, 26.2MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|█████████▉| 4.97G/4.98G [03:23<00:00, 28.3MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|█████████▉| 4.97G/4.98G [03:23<00:00, 28.8MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|█████████▉| 4.98G/4.98G [03:23<00:00, 27.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|██████████| 4.98G/4.98G [03:24<00:00, 24.3MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|██████████| 4.92G/4.92G [03:30<00:00, 23.3MB/s]\n",
      "model-00002-of-00004.safetensors: 100%|██████████| 5.00G/5.00G [03:38<00:00, 22.9MB/s]\n",
      "\n",
      "\n",
      "Upload 4 LFS files: 100%|██████████| 4/4 [03:39<00:00, 54.82s/it]\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HANMAC\\.cache\\huggingface\\hub\\models--chan1121--dacon4000. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/chan1121/dacon4000/commit/d9d86b3e5a594a86d1ab38bd70065eb60e2cc6b7', commit_message='Upload tokenizer', commit_description='', oid='d9d86b3e5a594a86d1ab38bd70065eb60e2cc6b7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")\n",
    "mergedModel.push_to_hub(\"chan1121/dacon4000\")\n",
    "tokenizer.push_to_hub(\"chan1121/dacon4000\", use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "def setup_llm_pipeline():\n",
    "    # 4비트 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # 모델 ID  chan1121/dacon-9b rtzr/ko-gemma-2-9b-it\n",
    "    tok = \"chan1121/dacon4000\"\n",
    "    model_id = \"chan1121/dacon4000\"  \n",
    "    # 토크나이저 로드 및 설정\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    # 모델 로드 및 양자화 설정 적용\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        do_sample=True)\n",
    "\n",
    "    # HuggingFacePipeline 객체 생성\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=400,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return hf\n",
    "\n",
    "# LLM 파이프라인\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]\n",
      "d:\\ai_project\\chanyoung\\dacon\\dacon\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # 모델 ID  chan1121/dacon-9b rtzr/ko-gemma-2-9b-it\n",
    "tok = \"chan1121/dacon4000\"\n",
    "model_id = \"chan1121/dacon4000\"  \n",
    "    # 토크나이저 로드 및 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    # 모델 로드 및 양자화 설정 적용\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        do_sample=True)\n",
    "\n",
    "    # HuggingFacePipeline 객체 생성\n",
    "text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=400,\n",
    "        #pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id = terminators,\n",
    "    )\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 한국의 수도는 서울입니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    ### 질문:\n",
    "    {question}\n",
    "    \n",
    "    질문의 핵심만 파악하여 간결하게 1-2문장으로 답변하고, 불필요한 설명은 피하며 요구된 정보만 제공하세요.\n",
    "    \n",
    "    ### 답변:\n",
    "\n",
    "    <|eot_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "prompt = PromptTemplate.from_template(template) \n",
    "hf.invoke(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
